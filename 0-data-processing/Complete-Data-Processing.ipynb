{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Training Dataset from All the exported chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"whatsapp_processing.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class WhatsAppProcessor:\n",
    "    def __init__(self, your_name: str, api_key: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the WhatsApp chat processor\n",
    "        \n",
    "        Args:\n",
    "            your_name (str): Your name in the chats (to identify which messages are yours)\n",
    "            api_key (str, optional): Gemini API key. Defaults to environment variable.\n",
    "        \"\"\"\n",
    "        self.your_name = your_name\n",
    "        self.api_key = api_key or os.environ.get(\"GEMINI_API_KEY\")\n",
    "        self.client = genai.Client(api_key=self.api_key)\n",
    "        self.model = \"gemini-2.0-flash\"\n",
    "        \n",
    "        # Rate limit tracking\n",
    "        self.request_timestamps = []\n",
    "        self.max_rpm = 14  # Setting slightly below actual limit for safety\n",
    "        self.requests_today = 0\n",
    "        self.max_rpd = 1400  # Setting slightly below actual limit for safety\n",
    "        \n",
    "        # Make sure directories exist\n",
    "        os.makedirs(\"whatsapp_data/processed\", exist_ok=True)\n",
    "        \n",
    "    def _respect_rate_limit(self):\n",
    "        \"\"\"Respect the Gemini API rate limits\"\"\"\n",
    "        # Check and enforce daily limit\n",
    "        current_date = datetime.now().date()\n",
    "        # Reset daily counter if it's a new day\n",
    "        if hasattr(self, 'last_request_date') and self.last_request_date != current_date:\n",
    "            self.requests_today = 0\n",
    "        self.last_request_date = current_date\n",
    "        \n",
    "        if self.requests_today >= self.max_rpd:\n",
    "            logger.warning(f\"Daily request limit reached ({self.max_rpd}). Waiting until tomorrow.\")\n",
    "            # Calculate seconds until midnight\n",
    "            now = datetime.now()\n",
    "            tomorrow = datetime(now.year, now.month, now.day) + timedelta(days=1)\n",
    "            seconds_to_wait = (tomorrow - now).total_seconds() + 10  # Add 10 seconds buffer\n",
    "            time.sleep(seconds_to_wait)\n",
    "            self.requests_today = 0\n",
    "            return\n",
    "        \n",
    "        # Check and enforce per-minute limit\n",
    "        current_time = time.time()\n",
    "        # Remove timestamps older than 1 minute\n",
    "        self.request_timestamps = [ts for ts in self.request_timestamps if current_time - ts < 60]\n",
    "        \n",
    "        if len(self.request_timestamps) >= self.max_rpm:\n",
    "            # Wait until we're under the rate limit\n",
    "            sleep_time = 60 - (current_time - self.request_timestamps[0]) + 1  # Add 1 second buffer\n",
    "            logger.info(f\"Rate limit approaching. Waiting {sleep_time:.2f} seconds\")\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        # Add current timestamp and increment counters\n",
    "        self.request_timestamps.append(time.time())\n",
    "        self.requests_today += 1\n",
    "\n",
    "    def parse_raw_chat(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Parse a raw WhatsApp chat export file\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the chat export file\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of message dictionaries with timestamp, sender, and text\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            # Try another common encoding\n",
    "            with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                content = f.read()\n",
    "                \n",
    "        # WhatsApp date format: DD/MM/YYYY, HH:MM - Sender: Message\n",
    "        # or: DD/MM/YYYY, HH:MM am/pm - Sender: Message\n",
    "        pattern = r'(\\d{1,2}/\\d{1,2}/\\d{2,4},\\s\\d{1,2}:\\d{2}(?:\\s?[ap]m)?)\\s-\\s([^:]+):\\s(.*)'\n",
    "        \n",
    "        # Split by new message (starting with date)\n",
    "        messages = []\n",
    "        current_message = None\n",
    "        \n",
    "        for line in content.split('\\n'):\n",
    "            match = re.match(pattern, line)\n",
    "            if match:\n",
    "                if current_message:\n",
    "                    messages.append(current_message)\n",
    "                \n",
    "                timestamp_str, sender, text = match.groups()\n",
    "                # Clean up sender name\n",
    "                sender = sender.strip()\n",
    "                \n",
    "                try:\n",
    "                    # Try to parse timestamp in various formats\n",
    "                    formats = [\n",
    "                        '%d/%m/%Y, %I:%M %p',  # 12-hour with AM/PM\n",
    "                        '%d/%m/%Y, %H:%M',     # 24-hour\n",
    "                        '%m/%d/%Y, %I:%M %p',  # US format 12-hour\n",
    "                        '%m/%d/%Y, %H:%M',     # US format 24-hour\n",
    "                    ]\n",
    "                    \n",
    "                    timestamp = None\n",
    "                    for fmt in formats:\n",
    "                        try:\n",
    "                            timestamp = datetime.strptime(timestamp_str, fmt)\n",
    "                            break\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                    \n",
    "                    if timestamp is None:\n",
    "                        # Fallback if no format worked\n",
    "                        timestamp = datetime.now()\n",
    "                        logger.warning(f\"Could not parse timestamp: {timestamp_str}, using current time\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error parsing timestamp: {timestamp_str}, {str(e)}\")\n",
    "                    timestamp = datetime.now()\n",
    "                \n",
    "                current_message = {\n",
    "                    'timestamp': timestamp,\n",
    "                    'sender': sender,\n",
    "                    'text': text\n",
    "                }\n",
    "            elif current_message:\n",
    "                # Continuation of previous message\n",
    "                current_message['text'] += f\"\\n{line}\"\n",
    "        \n",
    "        # Add the last message\n",
    "        if current_message:\n",
    "            messages.append(current_message)\n",
    "            \n",
    "        return messages\n",
    "\n",
    "    def group_messages_by_conversation(self, messages: List[Dict], max_time_diff: int = 3600) -> List[List[Dict]]:\n",
    "        \"\"\"\n",
    "        Group messages into conversations based on time gaps\n",
    "        \n",
    "        Args:\n",
    "            messages (List[Dict]): List of message dictionaries\n",
    "            max_time_diff (int): Maximum time difference in seconds between messages in the same conversation\n",
    "            \n",
    "        Returns:\n",
    "            List[List[Dict]]: List of conversations, each containing a list of message dictionaries\n",
    "        \"\"\"\n",
    "        if not messages:\n",
    "            return []\n",
    "            \n",
    "        conversations = []\n",
    "        current_conversation = [messages[0]]\n",
    "        \n",
    "        for i in range(1, len(messages)):\n",
    "            time_diff = (messages[i]['timestamp'] - messages[i-1]['timestamp']).total_seconds()\n",
    "            \n",
    "            # If time difference is greater than threshold, start a new conversation\n",
    "            if time_diff > max_time_diff:\n",
    "                if len(current_conversation) > 1:  # Only keep conversations with at least 2 messages\n",
    "                    conversations.append(current_conversation)\n",
    "                current_conversation = []\n",
    "                \n",
    "            current_conversation.append(messages[i])\n",
    "        \n",
    "        # Add the last conversation if it has at least 2 messages\n",
    "        if len(current_conversation) > 1:\n",
    "            conversations.append(current_conversation)\n",
    "            \n",
    "        return conversations\n",
    "\n",
    "    def format_conversation_for_llm(self, conversation: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Format a conversation for input to the LLM\n",
    "        \n",
    "        Args:\n",
    "            conversation (List[Dict]): List of message dictionaries in a conversation\n",
    "            \n",
    "        Returns:\n",
    "            str: Formatted conversation string\n",
    "        \"\"\"\n",
    "        formatted = \"Convert this WhatsApp conversation into a training example for an LLM fine-tuning dataset. \"\n",
    "        formatted += f\"Messages from '{self.your_name}' should be assigned the 'model' role, and messages from others should be the 'user' role.\\n\\n\"\n",
    "        formatted += \"Format the output as a valid JSON object following this structure:\\n\"\n",
    "        formatted += '{\\n  \"contents\": [\\n    {\"role\": \"user/model\", \"parts\": [{\"text\": \"message content\"}]},\\n    ...\\n  ]\\n}\\n\\n'\n",
    "        formatted += \"Combine sequential messages from the same speaker with a line break between them. Preserve all emojis, slang, and casual language. Output only valid JSON, no explanations.\\n\\n\"\n",
    "        formatted += \"CONVERSATION START:\\n\"\n",
    "        \n",
    "        for msg in conversation:\n",
    "            timestamp_str = msg['timestamp'].strftime('%I:%M %p')\n",
    "            formatted += f\"{timestamp_str} - {msg['sender']}: {msg['text']}\\n\"\n",
    "        \n",
    "        formatted += \"CONVERSATION END\\n\"\n",
    "        return formatted\n",
    "\n",
    "    def process_conversation_with_llm(self, conversation_text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a conversation with Gemini to convert it to the training format\n",
    "        \n",
    "        Args:\n",
    "            conversation_text (str): Formatted conversation text\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Processed conversation in the training format\n",
    "        \"\"\"\n",
    "        self._respect_rate_limit()\n",
    "        \n",
    "        try:\n",
    "            contents = [\n",
    "                types.Content(\n",
    "                    role=\"user\",\n",
    "                    parts=[types.Part.from_text(text=conversation_text)],\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            generate_content_config = types.GenerateContentConfig(\n",
    "                response_mime_type=\"application/json\",\n",
    "            )\n",
    "            \n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.model,\n",
    "                contents=contents,\n",
    "                config=generate_content_config,\n",
    "            )\n",
    "            \n",
    "            # Parse the response as JSON\n",
    "            try:\n",
    "                result = json.loads(response.text)\n",
    "                return result\n",
    "            except json.JSONDecodeError:\n",
    "                # If parsing fails, try to extract JSON from the response\n",
    "                logger.warning(\"Failed to parse LLM response as JSON, attempting to extract JSON\")\n",
    "                json_pattern = r'```json\\s*(.*?)\\s*```'\n",
    "                json_match = re.search(json_pattern, response.text, re.DOTALL)\n",
    "                \n",
    "                if json_match:\n",
    "                    try:\n",
    "                        result = json.loads(json_match.group(1))\n",
    "                        return result\n",
    "                    except json.JSONDecodeError:\n",
    "                        logger.error(\"Failed to extract valid JSON from code block\")\n",
    "                \n",
    "                # Another attempt with just finding the first { and last }\n",
    "                try:\n",
    "                    start_idx = response.text.find('{')\n",
    "                    end_idx = response.text.rfind('}') + 1\n",
    "                    if start_idx != -1 and end_idx > start_idx:\n",
    "                        json_str = response.text[start_idx:end_idx]\n",
    "                        result = json.loads(json_str)\n",
    "                        return result\n",
    "                except Exception:\n",
    "                    pass\n",
    "                    \n",
    "                logger.error(f\"Failed to parse LLM response as JSON: {response.text}\")\n",
    "                return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calling Gemini API: {str(e)}\")\n",
    "            time.sleep(5)  # Back off on errors\n",
    "            return None\n",
    "\n",
    "    def validate_training_example(self, example: Dict) -> bool:\n",
    "        \"\"\"\n",
    "        Validate that a training example has the correct structure\n",
    "        \n",
    "        Args:\n",
    "            example (Dict): Training example dictionary\n",
    "            \n",
    "        Returns:\n",
    "            bool: Whether the example is valid\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not example or 'contents' not in example:\n",
    "                return False\n",
    "                \n",
    "            contents = example['contents']\n",
    "            if not isinstance(contents, list) or len(contents) < 1:  # Allow single messages\n",
    "                return False\n",
    "                \n",
    "            # Check message structure\n",
    "            for content in contents:\n",
    "                if 'role' not in content or 'parts' not in content:\n",
    "                    return False\n",
    "                if not content['parts'] or not isinstance(content['parts'], list):\n",
    "                    return False\n",
    "                if 'text' not in content['parts'][0]:\n",
    "                    return False\n",
    "                    \n",
    "                # Validate role is either 'user' or 'model'\n",
    "                if content['role'] not in ['user', 'model']:\n",
    "                    return False\n",
    "                    \n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def process_all_chats(self, output_path: str = \"whatsapp_data/processed/train_data.jsonl\"):\n",
    "        \"\"\"\n",
    "        Process all WhatsApp chat exports in the raw_chats folder\n",
    "        \n",
    "        Args:\n",
    "            output_path (str): Path to save the processed JSONL file\n",
    "        \"\"\"\n",
    "        chat_files = glob.glob(\"whatsapp_data/raw_chats/*.txt\")\n",
    "        if not chat_files:\n",
    "            logger.error(\"No chat files found in whatsapp_data/raw_chats/\")\n",
    "            return\n",
    "            \n",
    "        logger.info(f\"Found {len(chat_files)} chat files to process\")\n",
    "        all_training_examples = []\n",
    "        \n",
    "        for file_path in tqdm(chat_files, desc=\"Processing chat files\"):\n",
    "            try:\n",
    "                logger.info(f\"Processing {file_path}\")\n",
    "                \n",
    "                # Parse the raw chat\n",
    "                messages = self.parse_raw_chat(file_path)\n",
    "                logger.info(f\"Found {len(messages)} messages in {file_path}\")\n",
    "                \n",
    "                # Group messages into conversations\n",
    "                conversations = self.group_messages_by_conversation(messages)\n",
    "                logger.info(f\"Grouped into {len(conversations)} conversations\")\n",
    "                \n",
    "                # Process each conversation\n",
    "                for conv_idx, conversation in enumerate(conversations):\n",
    "                    # Process all conversations, including single messages\n",
    "                    logger.info(f\"Processing conversation {conv_idx+1}/{len(conversations)} with {len(conversation)} messages\")\n",
    "                    \n",
    "                    # Format the conversation for the LLM\n",
    "                    conversation_text = self.format_conversation_for_llm(conversation)\n",
    "                    \n",
    "                    # Process with Gemini\n",
    "                    training_example = self.process_conversation_with_llm(conversation_text)\n",
    "                    \n",
    "                    # Validate and add to results\n",
    "                    if training_example and self.validate_training_example(training_example):\n",
    "                        all_training_examples.append(training_example)\n",
    "                    else:\n",
    "                        logger.warning(f\"Invalid training example for conversation {conv_idx}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        \n",
    "        # Save the results\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for example in all_training_examples:\n",
    "                f.write(json.dumps(example) + '\\n')\n",
    "                \n",
    "        logger.info(f\"Saved {len(all_training_examples)} training examples to {output_path}\")\n",
    "\n",
    "def main():\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Get values from environment variables\n",
    "    your_name = os.getenv('YOUR_NAME')\n",
    "    api_key = os.getenv('GEMINI_API_KEY')\n",
    "    \n",
    "    if not your_name:\n",
    "        raise ValueError(\"YOUR_NAME environment variable is not set\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"GEMINI_API_KEY environment variable is not set\")\n",
    "    \n",
    "    # Set fixed output path\n",
    "    output_path = \"whatsapp_data/processed/train_data.jsonl\"\n",
    "    \n",
    "    processor = WhatsAppProcessor(your_name=your_name, api_key=api_key)\n",
    "    processor.process_all_chats(output_path=output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean out lines \n",
    "### To Ensure that each entry contains both user & model roles at least once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def clean_jsonl(input_file, output_file):\n",
    "    # Read the input file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Process each line\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            # Check if the line has both user and model roles\n",
    "            has_user = False\n",
    "            has_model = False\n",
    "            \n",
    "            for content in data.get('contents', []):\n",
    "                if content.get('role') == 'user':\n",
    "                    has_user = True\n",
    "                elif content.get('role') == 'model':\n",
    "                    has_model = True\n",
    "                \n",
    "                # If we found both roles, we can stop checking\n",
    "                if has_user and has_model:\n",
    "                    break\n",
    "            \n",
    "            # Only keep lines that have both roles\n",
    "            if has_user and has_model:\n",
    "                cleaned_lines.append(line)\n",
    "                \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error parsing line: {line}\")\n",
    "            continue\n",
    "    \n",
    "    # Write the cleaned data to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(cleaned_lines)\n",
    "    \n",
    "    print(f\"Original lines: {len(lines)}\")\n",
    "    print(f\"Cleaned lines: {len(cleaned_lines)}\")\n",
    "    print(f\"Removed lines: {len(lines) - len(cleaned_lines)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"whatsapp_data/clean/train_data_emjFixed.jsonl\"\n",
    "    output_file = \"whatsapp_data/clean/train_data_emjFixed_cleaned.jsonl\"\n",
    "    clean_jsonl(input_file, output_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Emojies\n",
    "Emojies in The LLM Output is in the unicode format. This will render out unicode formatted emojies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_jsonl(file_path):\n",
    "    \"\"\"Validate the JSONL file format.\"\"\"\n",
    "    valid_count = 0\n",
    "    invalid_lines = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "                # Check if entry has the expected structure\n",
    "                if (\n",
    "                    \"contents\" in entry and\n",
    "                    len(entry[\"contents\"]) == 2 and\n",
    "                    entry[\"contents\"][0][\"role\"] == \"user\" and\n",
    "                    entry[\"contents\"][1][\"role\"] == \"model\"\n",
    "                ):\n",
    "                    valid_count += 1\n",
    "                else:\n",
    "                    invalid_lines.append((i+1, \"Invalid structure\"))\n",
    "            except json.JSONDecodeError:\n",
    "                invalid_lines.append((i+1, \"Invalid JSON\"))\n",
    "    \n",
    "    return f\"Valid: {valid_count}, Invalid: {len(invalid_lines)}\", invalid_lines\n",
    "\n",
    "file_path = \"whatsapp_data/processed/training_data.jsonl\"\n",
    "\n",
    "validate_jsonl(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review samples to ensure conversation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_samples(file_path, n=10):\n",
    "    \"\"\"Print n random samples from the JSONL file for manual review.\"\"\"\n",
    "    import random\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    sample_count = min(n, len(lines))\n",
    "    samples = random.sample(lines, sample_count)\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        entry = json.loads(sample)\n",
    "        user_text = entry[\"contents\"][0][\"parts\"][0][\"text\"]\n",
    "        model_text = entry[\"contents\"][1][\"parts\"][0][\"text\"]\n",
    "        \n",
    "        print(f\"=== Sample {i+1} ===\")\n",
    "        print(f\"User: {user_text}\")\n",
    "        print(f\"Model: {model_text}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "review_samples(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into Training & Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def split_jsonl_file(input_file, train_ratio=0.8, random_seed=42):\n",
    "    \"\"\"\n",
    "    Split a JSONL file into training and evaluation sets.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the input JSONL file\n",
    "        train_ratio (float): Ratio of data to use for training (default: 0.8)\n",
    "        random_seed (int): Random seed for reproducibility (default: 42)\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = Path(\"whatsapp_data/final\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Read all lines from the input file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Shuffle the lines\n",
    "    random.shuffle(lines)\n",
    "    \n",
    "    # Calculate split point\n",
    "    split_idx = int(len(lines) * train_ratio)\n",
    "    \n",
    "    # Split into train and eval sets\n",
    "    train_lines = lines[:split_idx]\n",
    "    eval_lines = lines[split_idx:]\n",
    "    \n",
    "    # Write training set\n",
    "    train_file = output_dir / \"train_data.jsonl\"\n",
    "    with open(train_file, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(train_lines)\n",
    "    \n",
    "    # Write evaluation set\n",
    "    eval_file = output_dir / \"eval_data.jsonl\"\n",
    "    with open(eval_file, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(eval_lines)\n",
    "    \n",
    "    print(f\"Total samples: {len(lines)}\")\n",
    "    print(f\"Training samples: {len(train_lines)}\")\n",
    "    print(f\"Evaluation samples: {len(eval_lines)}\")\n",
    "    print(f\"\\nFiles written to:\")\n",
    "    print(f\"Training: {train_file}\")\n",
    "    print(f\"Evaluation: {eval_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"whatsapp_data/processed/training_data.jsonl\"\n",
    "    split_jsonl_file(input_file) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
